{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key looks good so far\n"
     ]
    }
   ],
   "source": [
    "# set up environment\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if api_key and api_key.startswith('sk-proj-') and len(api_key)>10:\n",
    "    print(\"API key looks good so far\")\n",
    "else:\n",
    "    print(\"There might be a problem with your API key? Please visit the troubleshooting notebook!\")\n",
    "    \n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\"\n",
    "question = \"\"\"\n",
    "How do I use the ollama chat python library to stream a response?\n",
    "\"\"\"\n",
    "system_prompt = \"\"\"\n",
    "You are an expert in computer science, machine learning, AI, Information Technology and programming in any language. \n",
    "Answer the user question with examples if appropriate.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d01582b2-6c0f-468e-8ec5-a48e1021b1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_answer(question):\n",
    "    if MODEL == MODEL_GPT:\n",
    "        stream = openai.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "              ],\n",
    "            stream=True\n",
    "        )\n",
    "        response = \"\"\n",
    "        display_handle = display(Markdown(\"\"), display_id=True)\n",
    "        for chunk in stream:\n",
    "            response += chunk.choices[0].delta.content or ''\n",
    "            response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "            update_display(Markdown(response), display_id=display_handle.display_id)\n",
    "\n",
    "    else:\n",
    "        for chunk in ollama.chat(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ],\n",
    "            stream=True\n",
    "        ):\n",
    "            if 'message' in chunk and 'content' in chunk['message']:\n",
    "                print(chunk['message']['content'], end='', flush=True) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The `ollama chat` Python library is used for interacting with conversational AI models provided by the Ollama platform. Streaming responses allow you to receive outputs from the model as they are being generated rather than waiting for the entire response to be completed.\n",
       "\n",
       "Here's a brief overview of how you can use this library to stream a response:\n",
       "\n",
       "1. **Installation**: Make sure you have the `ollama` library installed in your Python environment. If it’s not installed, you can install it using `pip`:\n",
       "\n",
       "   bash\n",
       "   pip install ollama\n",
       "   \n",
       "\n",
       "2. **Basic Usage**:\n",
       "   Here's an example of how to use the `ollama` library to stream responses:\n",
       "\n",
       "   python\n",
       "   import ollama\n",
       "\n",
       "   # Create a chat session with the specified model\n",
       "   chat_session = ollama.Chat(model=\"some-model-name\")\n",
       "\n",
       "   # Define a function to handle the streaming response\n",
       "   def stream_response():\n",
       "       # Send a message to the model and stream the response\n",
       "       for response in chat_session.stream(\"Hello, how are you?\"):\n",
       "           print(response)  # Each response is printed as it's received\n",
       "\n",
       "   # Execute the streaming response function\n",
       "   stream_response()\n",
       "   \n",
       "\n",
       "### Explanation of the Code:\n",
       "\n",
       "1. **Importing Library**: The first line imports the `ollama` library, which gives you access to its functionality.\n",
       "\n",
       "2. **Creating a Chat Session**: The `ollama.Chat(model=\"some-model-name\")` line initializes a chat session with a specific model (in this case, `\"some-model-name\"`). Make sure to replace `\"some-model-name\"` with the actual name of the model you want to use.\n",
       "\n",
       "3. **Streaming Responses**: The function `stream_response()` uses a for-loop to iterate through the generator returned by `chat_session.stream()`. The string `\"Hello, how are you?\"` is sent to the model for processing. The response is obtained piece by piece (as it is generated), allowing for a more dynamic interaction.\n",
       "\n",
       "4. **Printing Responses**: Each `response` received in the loop is printed out immediately, providing a real-time feel to the conversation rather than waiting for the entire response to be generated.\n",
       "\n",
       "### Why Use Streaming?\n",
       "\n",
       "- **Real-Time Interaction**: Streaming allows users to see responses in real-time, making the interaction feel more natural and engaging.\n",
       "- **Performance**: It can be more efficient as you don’t have to wait for the entire response to be formulated before starting to process it or displaying it to the user.\n",
       "- **User Experience**: In applications like chatbots or AI assistants, users appreciate immediate feedback, even if it's just part of a response.\n",
       "\n",
       "This basic example should help you get started with using the `ollama chat` library for streaming responses in your Python applications. Adjust the model name and prompts according to your specific use case!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "MODEL = MODEL_GPT\n",
    "\n",
    "stream_answer(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Llama 3.2 to answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72c30dc6-49b7-4ec4-83a5-131ca3d02914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `ollama` library is not a widely known or maintained library, so I'll provide an example of how you can create a simple text-based chatbot using Python.\n",
      "\n",
      "**What is Ollama?**\n",
      "\n",
      "I couldn't find any information on the `ollama` library. It's possible that it's a custom or proprietary library, or it might be a typo. If you meant to ask about another library like `Rasa` or `NLTK`, I'd be happy to help.\n",
      "\n",
      "**Streaming a response with Python**\n",
      "\n",
      "Assuming we're using a different library for building our chatbot, here's an example of how to create a simple text-based chatbot that streams responses:\n",
      "\n",
      "```python\n",
      "import time\n",
      "\n",
      "class ChatBot:\n",
      "    def __init__(self):\n",
      "        self.responses = {\n",
      "            \"hello\": [\"Hi there!\", \"Hey, what's up?\", \"Hello!\"],\n",
      "            \"goodbye\": [\"See you later!\", \"Bye for now!\", \"Goodbye!\"]\n",
      "        }\n",
      "\n",
      "    def process_input(self, input_str):\n",
      "        # Convert to lowercase and remove punctuation\n",
      "        input_str = input_str.lower()\n",
      "        input_str = ''.join(e for e in input_str if e.isalnum() or e.isspace())\n",
      "\n",
      "        # Check if the input is recognized\n",
      "        if input_str in self.responses:\n",
      "            return random.choice(self.responses[input_str])\n",
      "        else:\n",
      "            return \"I didn't understand that.\"\n",
      "\n",
      "def main():\n",
      "    bot = ChatBot()\n",
      "\n",
      "    while True:\n",
      "        user_input = input(\"User: \")\n",
      "        response = bot.process_input(user_input)\n",
      "        print(f\"Chatbot: {response}\")\n",
      "        time.sleep(1)  # wait for the next input\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "```\n",
      "\n",
      "In this example, we create a `ChatBot` class that stores possible responses to common inputs. The `process_input` method takes user input, processes it (e.g., converts to lowercase), and returns a response from the `responses` dictionary.\n",
      "\n",
      "The `main` function creates an instance of the `ChatBot`, enters an infinite loop where it prompts the user for input, processes the input using `process_input`, and prints the response. The program waits for 1 second before prompting the user again.\n",
      "\n",
      "To stream responses, you can use this code as a starting point, but keep in mind that it's a simple example and might not cover all edge cases or provide the most engaging chat experience.\n",
      "\n",
      "If you meant to ask about using `Rasa` or another library, I'd be happy to help with that!"
     ]
    }
   ],
   "source": [
    "MODEL = MODEL_LLAMA\n",
    "stream_answer(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958fdbda-f3fe-4335-9214-51ff1ba3e02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your question: How many times does the letter 'a' appear in this sentence?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question is: 0 How many times does the letter 'a' appear in this sentence?\n",
      "Let me count for you.\n",
      "\n",
      "The letter 'a' appears 7 times in the given sentence. \n",
      "\n",
      "Here's the breakdown:\n",
      "\n",
      "1. a\n",
      "2. an\n",
      "3. is\n",
      "4. ap\n",
      "5. pe\n",
      "6. ra\n",
      "7. and \n",
      "\n",
      "Note that some letters appear multiple times in a word, but I counted each occurrence separately."
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    my_question = input(\"Please enter your question:\")\n",
    "    print(f\"The question is: {0}\", my_question)\n",
    "    stream_answer(my_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3c16bc-374f-4be6-a297-307e88739993",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
